{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UDACITY - FUNDAMENTOS DE DATA SCIENCE II  \n",
    "#### PROJETO FINAL - Identificar fraude no Email da Enron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, time\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "import tester\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Select what features you'll use.\n",
    "features_list is a list of strings, each of which is a feature name. The first feature must be \"poi\". You will need to use more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "\n",
    "# Atributo principal\n",
    "main_feature = 'poi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Localiza características disponíveis\n",
    "features_available = []\n",
    "first_person = next(iter(data_dict))\n",
    "for feature in data_dict[first_person]:\n",
    "    if feature != 'poi': \n",
    "        features_available.append(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número total de características disponíveis: 21\n",
      "\n",
      "Características disponíveis: ['poi', 'salary', 'to_messages', 'deferral_payments', 'total_payments', 'exercised_stock_options', 'bonus', 'restricted_stock', 'shared_receipt_with_poi', 'restricted_stock_deferred', 'total_stock_value', 'expenses', 'loan_advances', 'from_messages', 'other', 'from_this_person_to_poi', 'director_fees', 'deferred_income', 'long_term_incentive', 'email_address', 'from_poi_to_this_person']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lista de características\n",
    "features_list = [main_feature] + features_available\n",
    "n_features = len(features_list)\n",
    "\n",
    "print 'Número total de características disponíveis:', n_features\n",
    "print\n",
    "\n",
    "print 'Características disponíveis:', features_list\n",
    "print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Características disponíveis\n",
    "\n",
    " 'poi', 'salary', 'to_messages', 'deferral_payments', 'total_payments', 'exercised_stock_options',\n",
    " 'bonus', 'restricted_stock', 'shared_receipt_with_poi', 'restricted_stock_deferred', 'total_stock_value',\n",
    " 'expenses', 'loan_advances', 'from_messages', 'other', 'from_this_person_to_poi', 'director_fees',\n",
    " 'deferred_income', 'long_term_incentive', 'email_address', 'from_poi_to_this_person'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de características selecionados: 20\n",
      "\n",
      "Características selecionados: ['poi', 'salary', 'to_messages', 'deferral_payments', 'total_payments', 'exercised_stock_options', 'bonus', 'restricted_stock', 'shared_receipt_with_poi', 'restricted_stock_deferred', 'total_stock_value', 'expenses', 'loan_advances', 'from_messages', 'other', 'from_this_person_to_poi', 'director_fees', 'deferred_income', 'long_term_incentive', 'from_poi_to_this_person']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Características selecionadas.\n",
    "features_list = ['poi', 'salary', 'to_messages', 'deferral_payments', 'total_payments', 'exercised_stock_options',\n",
    "                  'bonus', 'restricted_stock', 'shared_receipt_with_poi', 'restricted_stock_deferred', 'total_stock_value',\n",
    "                  'expenses', 'loan_advances', 'from_messages', 'other', 'from_this_person_to_poi', 'director_fees',\n",
    "                  'deferred_income', 'long_term_incentive','from_poi_to_this_person']\n",
    "\n",
    "n_features = len(features_list)\n",
    "\n",
    "print 'Número de características selecionados:', n_features\n",
    "print\n",
    "\n",
    "print 'Características selecionados:', features_list\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nan_check(data_dict):\n",
    "    # Verifica existência de valores 'NaN' nas características selecionadas\n",
    "    print '### Checando valores \"NaN\" ###'\n",
    "    nan_dict = {}\n",
    "    for key in features_list:\n",
    "        nan_dict.update({key:0})\n",
    "\n",
    "    for data_point in data_dict:\n",
    "        for feature in data_dict[data_point]:\n",
    "            if feature in features_list:\n",
    "                if data_dict[data_point][feature] == 'NaN':\n",
    "                    nan_dict[feature] += 1\n",
    "\n",
    "    print 'Quantidade de valores \"NaN\" nas característica selecionada:'\n",
    "    for feature in nan_dict:\n",
    "        print feature+'= ', nan_dict[feature]\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nan_to_zero(data_dict):\n",
    "    # Troca valores NaN por 0\n",
    "    print '### Trocando valores \"NaN\" por 0 ###'\n",
    "    for data_point in data_dict:\n",
    "        for feature in data_dict[data_point]:\n",
    "            if data_dict[data_point][feature] == 'NaN':\n",
    "                data_dict[data_point][feature] = 0\n",
    "    print 'Concluído. OK!'\n",
    "    print\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def financial_values_check(data_dict):\n",
    "    # Verifica valores financeiros\n",
    "    erro = 0\n",
    "    print '### Checando se a soma dos valores de pagamento é igual ao total de valores de pagamento ###'\n",
    "    for data_point in data_dict:\n",
    "        total = (data_dict[data_point]['salary'] +\n",
    "                 data_dict[data_point]['bonus'] +\n",
    "                 data_dict[data_point]['long_term_incentive'] +\n",
    "                 data_dict[data_point]['deferred_income'] +\n",
    "                 data_dict[data_point]['deferral_payments'] +\n",
    "                 data_dict[data_point]['loan_advances'] +\n",
    "                 data_dict[data_point]['other'] +\n",
    "                 data_dict[data_point]['expenses'] +\n",
    "                 data_dict[data_point]['director_fees'])\n",
    "        if total != data_dict[data_point]['total_payments']:\n",
    "            erro += 1\n",
    "            print 'Erro:', data_point\n",
    "            print data_dict[data_point]\n",
    "            print\n",
    "    if erro == 0:\n",
    "        print 'As somas dos valores de pagamento parecem certas agora.'\n",
    "        print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Checando valores \"NaN\" ###\n",
      "Quantidade de valores \"NaN\" nas característica selecionada:\n",
      "salary=  51\n",
      "to_messages=  60\n",
      "deferral_payments=  107\n",
      "total_payments=  21\n",
      "loan_advances=  142\n",
      "bonus=  64\n",
      "restricted_stock_deferred=  128\n",
      "total_stock_value=  20\n",
      "shared_receipt_with_poi=  60\n",
      "long_term_incentive=  80\n",
      "exercised_stock_options=  44\n",
      "from_messages=  60\n",
      "other=  53\n",
      "from_poi_to_this_person=  60\n",
      "from_this_person_to_poi=  60\n",
      "poi=  0\n",
      "deferred_income=  97\n",
      "expenses=  51\n",
      "restricted_stock=  36\n",
      "director_fees=  129\n",
      "\n",
      "### Trocando valores \"NaN\" por 0 ###\n",
      "Concluído. OK!\n",
      "\n",
      "### Checando valores \"NaN\" ###\n",
      "Quantidade de valores \"NaN\" nas característica selecionada:\n",
      "salary=  0\n",
      "to_messages=  0\n",
      "deferral_payments=  0\n",
      "total_payments=  0\n",
      "loan_advances=  0\n",
      "bonus=  0\n",
      "restricted_stock_deferred=  0\n",
      "total_stock_value=  0\n",
      "shared_receipt_with_poi=  0\n",
      "long_term_incentive=  0\n",
      "exercised_stock_options=  0\n",
      "from_messages=  0\n",
      "other=  0\n",
      "from_poi_to_this_person=  0\n",
      "from_this_person_to_poi=  0\n",
      "poi=  0\n",
      "deferred_income=  0\n",
      "expenses=  0\n",
      "restricted_stock=  0\n",
      "director_fees=  0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verifica existência de valores 'NaN' nas características selecionadas\n",
    "nan_check(data_dict)\n",
    "\n",
    "# Troca valores NaN por 0\n",
    "data_dict = nan_to_zero(data_dict)\n",
    "\n",
    "# Verifica novamente a existência de valores 'NaN' nas características selecionadas\n",
    "nan_check(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Número de data points: 146\n",
      "\n",
      "Total de POIs    : 18\n",
      "Total de Não-POIs: 128\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Conta número data points\n",
    "num_data_points = 0\n",
    "for data_points in data_dict.iteritems():\n",
    "    num_data_points += 1\n",
    "    name = data_points[0]\n",
    "\n",
    "print\n",
    "print 'Número de data points:', num_data_points\n",
    "print\n",
    "\n",
    "# Conta número de POIs e Não-POIs\n",
    "num_poi = 0\n",
    "num_npoi = 0\n",
    "for data_point in data_dict:\n",
    "    if data_dict[data_point]['poi']:\n",
    "        num_poi += 1\n",
    "    else:\n",
    "        num_npoi += 1\n",
    "\n",
    "print 'Total de POIs    :', num_poi\n",
    "print 'Total de Não-POIs:', num_npoi\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Checando se a soma dos valores de pagamento é igual ao total de valores de pagamento ###\n",
      "Erro: BELFER ROBERT\n",
      "{'salary': 0, 'to_messages': 0, 'deferral_payments': -102500, 'total_payments': 102500, 'exercised_stock_options': 3285, 'bonus': 0, 'restricted_stock': 0, 'shared_receipt_with_poi': 0, 'restricted_stock_deferred': 44093, 'total_stock_value': -44093, 'expenses': 0, 'loan_advances': 0, 'from_messages': 0, 'other': 0, 'from_this_person_to_poi': 0, 'poi': False, 'director_fees': 3285, 'deferred_income': 0, 'long_term_incentive': 0, 'email_address': 0, 'from_poi_to_this_person': 0}\n",
      "\n",
      "Erro: BHATNAGAR SANJAY\n",
      "{'salary': 0, 'to_messages': 523, 'deferral_payments': 0, 'total_payments': 15456290, 'exercised_stock_options': 2604490, 'bonus': 0, 'restricted_stock': -2604490, 'shared_receipt_with_poi': 463, 'restricted_stock_deferred': 15456290, 'total_stock_value': 0, 'expenses': 0, 'loan_advances': 0, 'from_messages': 29, 'other': 137864, 'from_this_person_to_poi': 1, 'poi': False, 'director_fees': 137864, 'deferred_income': 0, 'long_term_incentive': 0, 'email_address': 'sanjay.bhatnagar@enron.com', 'from_poi_to_this_person': 0}\n",
      "\n",
      "Foi detectada uma diferença nos valore financeiros.\n",
      "Ao verificar o arquivo \"enron1702insiderpay.pdf\" ficou claro que houve um deslocamento das informações.\n",
      "Possivelmente algum erro humano na entrada das informações no conjunto de dados.\n",
      "O erro foi corrigido programaticamente.\n",
      "\n",
      "Dados referentes a BELFER ROBERT corrigidos!\n",
      "\n",
      "Dados referentes a BHATNAGAR SANJAY corrigidos!\n",
      "\n",
      "### Checando se a soma dos valores de pagamento é igual ao total de valores de pagamento ###\n",
      "As somas dos valores de pagamento parecem certas agora.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verifica a soma dos valores de pagamento\n",
    "financial_values_check(data_dict)\n",
    "\n",
    "print 'Foi detectada uma diferença nos valore financeiros.'\n",
    "print 'Ao verificar o arquivo \"enron1702insiderpay.pdf\" ficou claro que houve um deslocamento das informações.'\n",
    "print 'Possivelmente algum erro humano na entrada das informações no conjunto de dados.'\n",
    "print 'O erro foi corrigido programaticamente.'\n",
    "print\n",
    "\n",
    "data_dict['BELFER ROBERT']['deferred_income'] = data_dict['BELFER ROBERT']['deferral_payments']\n",
    "data_dict['BELFER ROBERT']['deferral_payments'] = 0\n",
    "data_dict['BELFER ROBERT']['expenses'] = data_dict['BELFER ROBERT']['director_fees']\n",
    "data_dict['BELFER ROBERT']['director_fees'] = data_dict['BELFER ROBERT']['total_payments']\n",
    "data_dict['BELFER ROBERT']['total_payments'] = data_dict['BELFER ROBERT']['exercised_stock_options']\n",
    "data_dict['BELFER ROBERT']['exercised_stock_options'] = 0\n",
    "\n",
    "print 'Dados referentes a BELFER ROBERT corrigidos!'\n",
    "data_dict['BELFER ROBERT']\n",
    "print\n",
    "\n",
    "data_dict['BHATNAGAR SANJAY']['total_stock_value'] = data_dict['BHATNAGAR SANJAY']['restricted_stock_deferred']\n",
    "data_dict['BHATNAGAR SANJAY']['restricted_stock_deferred'] = data_dict['BHATNAGAR SANJAY']['restricted_stock']\n",
    "data_dict['BHATNAGAR SANJAY']['restricted_stock'] = data_dict['BHATNAGAR SANJAY']['exercised_stock_options']\n",
    "data_dict['BHATNAGAR SANJAY']['exercised_stock_options'] = data_dict['BHATNAGAR SANJAY']['total_payments']\n",
    "data_dict['BHATNAGAR SANJAY']['total_payments'] = data_dict['BHATNAGAR SANJAY']['director_fees']\n",
    "data_dict['BHATNAGAR SANJAY']['director_fees'] = data_dict['BHATNAGAR SANJAY']['expenses']\n",
    "data_dict['BHATNAGAR SANJAY']['expenses'] = data_dict['BHATNAGAR SANJAY']['other']\n",
    "data_dict['BHATNAGAR SANJAY']['other'] = 0\n",
    "\n",
    "print 'Dados referentes a BHATNAGAR SANJAY corrigidos!'\n",
    "data_dict['BHATNAGAR SANJAY']\n",
    "print\n",
    "\n",
    "# Verifica valores de pagamento\n",
    "financial_values_check(data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_salary_bonus(data_dict):\n",
    "    # Plota gráfico Salary x Bonus\n",
    "    for data_point in data_dict:\n",
    "        salary = data_dict[data_point]['salary']\n",
    "        bonus = data_dict[data_point]['bonus']\n",
    "        #if salary != 'NaN' and bonus != 'NaN':\n",
    "        plt.scatter( salary, bonus )\n",
    "\n",
    "    plt.xlabel(\"salary\")\n",
    "    plt.ylabel(\"bonus\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greater_salary(data_dict):\n",
    "    # Localiza data ponit com maior salário\n",
    "    print '### Localizando maior salário ###'\n",
    "    person_greater_salary = []\n",
    "    max_salary = 0\n",
    "    for data_point in data_dict:\n",
    "        salary = data_dict[data_point]['salary']\n",
    "        #if salary != 'NaN' and \n",
    "        if salary > max_salary:\n",
    "            max_salary = salary\n",
    "            person_greater_salary = data_point, '=', data_dict[data_point]\n",
    "    print 'Data point onde ocorre o maior salário:'\n",
    "    print person_greater_salary\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(outlier):\n",
    "    # Remove outliers\n",
    "    data_dict.pop(outlier, 0)\n",
    "    print 'Outlier removido:', outlier\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mark Frevert  \n",
    "Atuou como diretor executivo da Enron Europe de 1986 a 2000 e foi nomeado presidente da Enron em 2001.  \n",
    "Ele era um integrante importante da empresa, mas não foi considerado uma pessoa de interesse.  \n",
    "Acredito que ele era um funcionário acima média da Enron durante esse período por causa de sua compensação substancial e o removerá do conjunto de dados.\n",
    "\n",
    "#### John Baxter  \n",
    "Era um ex-vice-presidente da Enron e morreu de um aparente disparo auto-infligido antes de poder testemunhar contra outros executivos da Enron. Vou removê-lo do conjunto de dados, pois ele não é uma pessoa de interesse.\n",
    "\n",
    "#### John Lavorato  \n",
    "Era um executivo de alto escalão na divisão de comércio de energia da Enron e recebeu grandes bônus para não deixar a Enron.  \n",
    "Como ele não era uma pessoa de interesse, e o grande bônus acabou distorcendo sua remuneração total, acho que seria apropriado removê-lo do conjunto de dados.\n",
    "\n",
    "#### Lawrence Whalley  \n",
    "Serviu como presidente da Enron e demitiu Andrew Fastow quando ficou evidente a gravidade da situação da Enron. Ele foi investigado completamente, mas não identificado como uma pessoa de interesse e, portanto, será removido do conjunto de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No gráfico plotado foi percebido alguns pontos fora do padrão.\n",
      "\n",
      "### Localizando maior salário ###\n",
      "Data point onde ocorre o maior salário:\n",
      "('TOTAL', '=', {'salary': 26704229, 'to_messages': 0, 'deferral_payments': 32083396, 'total_payments': 309886585, 'exercised_stock_options': 311764000, 'bonus': 97343619, 'restricted_stock': 130322299, 'shared_receipt_with_poi': 0, 'restricted_stock_deferred': -7576788, 'total_stock_value': 434509511, 'expenses': 5235198, 'loan_advances': 83925000, 'from_messages': 0, 'other': 42667589, 'from_this_person_to_poi': 0, 'poi': False, 'director_fees': 1398517, 'deferred_income': -27992891, 'long_term_incentive': 48521928, 'email_address': 0, 'from_poi_to_this_person': 0})\n",
      "\n",
      "Ao checar o maior valor foi percebido um data point não útil para a análise com nome \"TOTAL\"\n",
      "Optou-se por remover o outlier.\n",
      "Outlier removido: TOTAL\n",
      "\n",
      "Novo gráfico após a remoção do outlier \"TOTAL\".\n",
      "\n",
      "### Localizando maior salário ###\n",
      "Data point onde ocorre o maior salário:\n",
      "('SKILLING JEFFREY K', '=', {'salary': 1111258, 'to_messages': 3627, 'deferral_payments': 0, 'total_payments': 8682716, 'exercised_stock_options': 19250000, 'bonus': 5600000, 'restricted_stock': 6843672, 'shared_receipt_with_poi': 2042, 'restricted_stock_deferred': 0, 'total_stock_value': 26093672, 'expenses': 29336, 'loan_advances': 0, 'from_messages': 108, 'other': 22122, 'from_this_person_to_poi': 30, 'poi': True, 'director_fees': 0, 'deferred_income': 0, 'long_term_incentive': 1920000, 'email_address': 'jeff.skilling@enron.com', 'from_poi_to_this_person': 88})\n",
      "\n",
      "O maior salário agora é relativo a uma pessoa.\n",
      "\n",
      "Ao observar o arquivo \"enron61702insiderpay.pdf\" foi percebido um data point não referente a uma pessoa.\n",
      "Por não parecer útil para esta análise optou-se por removê-lo: \"THE TRAVEL AGENCY IN THE PARK\"\n",
      "\n",
      "Outlier removido: THE TRAVEL AGENCY IN THE PARK\n",
      "\n",
      "Outlier removido: FREVERT MARK A\n",
      "\n",
      "Outlier removido: LAVORATO JOHN J\n",
      "\n",
      "Outlier removido: WHALLEY LAWRENCE G\n",
      "\n",
      "Outlier removido: BAXTER JOHN C\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verifica atributos Salary e Bonus\n",
    "print 'No gráfico plotado foi percebido alguns pontos fora do padrão.'\n",
    "print\n",
    "#scatter_salary_bonus(data_dict)\n",
    "\n",
    "greater_salary(data_dict)\n",
    "print 'Ao checar o maior valor foi percebido um data point não útil para a análise com nome \"TOTAL\"'\n",
    "print 'Optou-se por remover o outlier.'\n",
    "\n",
    "remove_outliers('TOTAL')\n",
    "\n",
    "# Verifica atributos Salary e Bonus\n",
    "\n",
    "print 'Novo gráfico após a remoção do outlier \"TOTAL\".'\n",
    "print\n",
    "#scatter_salary_bonus(data_dict)\n",
    "\n",
    "greater_salary(data_dict)\n",
    "print 'O maior salário agora é relativo a uma pessoa.'\n",
    "print\n",
    "\n",
    "print 'Ao observar o arquivo \"enron61702insiderpay.pdf\" foi percebido um data point não referente a uma pessoa.'\n",
    "print 'Por não parecer útil para esta análise optou-se por removê-lo: \"THE TRAVEL AGENCY IN THE PARK\"'\n",
    "print\n",
    "remove_outliers('THE TRAVEL AGENCY IN THE PARK')\n",
    "\n",
    "remove_outliers('FREVERT MARK A')\n",
    "remove_outliers('LAVORATO JOHN J')\n",
    "remove_outliers('WHALLEY LAWRENCE G')\n",
    "remove_outliers('BAXTER JOHN C')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Create new feature(s)  \n",
    "Store to my_dataset for easy export below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dataset = data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 140 entries, ALLEN PHILLIP K to YEAP SOON\n",
      "Data columns (total 20 columns):\n",
      "poi                          140 non-null bool\n",
      "salary                       140 non-null int64\n",
      "to_messages                  140 non-null int64\n",
      "deferral_payments            140 non-null int64\n",
      "total_payments               140 non-null int64\n",
      "exercised_stock_options      140 non-null int64\n",
      "bonus                        140 non-null int64\n",
      "restricted_stock             140 non-null int64\n",
      "shared_receipt_with_poi      140 non-null int64\n",
      "restricted_stock_deferred    140 non-null int64\n",
      "total_stock_value            140 non-null int64\n",
      "expenses                     140 non-null int64\n",
      "loan_advances                140 non-null int64\n",
      "from_messages                140 non-null int64\n",
      "other                        140 non-null int64\n",
      "from_this_person_to_poi      140 non-null int64\n",
      "director_fees                140 non-null int64\n",
      "deferred_income              140 non-null int64\n",
      "long_term_incentive          140 non-null int64\n",
      "from_poi_to_this_person      140 non-null int64\n",
      "dtypes: bool(1), int64(19)\n",
      "memory usage: 22.0+ KB\n"
     ]
    }
   ],
   "source": [
    "# Normalização\n",
    "\n",
    "df = pd.DataFrame.from_dict(my_dataset, orient='index')\n",
    "df = df[features_list]\n",
    "\n",
    "df.info()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df = df.copy()\n",
    "scaled_df.iloc[:,1:] = scale(scaled_df.iloc[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dataset = scaled_df.to_dict(orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lista com novos atributos: ['poi', 'salary', 'to_messages', 'deferral_payments', 'total_payments', 'exercised_stock_options', 'bonus', 'restricted_stock', 'shared_receipt_with_poi', 'restricted_stock_deferred', 'total_stock_value', 'expenses', 'loan_advances', 'from_messages', 'other', 'from_this_person_to_poi', 'director_fees', 'deferred_income', 'long_term_incentive', 'from_poi_to_this_person', 'to_poi_message_proportion', 'from_poi_message_proportion', 'shared_poi_message_proportion', 'bonus_to_salary', 'bonus_to_total']\n",
      "\n",
      "Total de atributos: 25\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Adiciona novas caracteristicas \n",
    "use_new_feature = True\n",
    "\n",
    "if use_new_feature:\n",
    "    #for person in my_dataset.values():\n",
    "    for person in my_dataset:\n",
    "        my_dataset[person]['to_poi_message_proportion'] = my_dataset[person]['from_this_person_to_poi']/my_dataset[person]['to_messages']\n",
    "        my_dataset[person]['from_poi_message_proportion'] = my_dataset[person]['from_poi_to_this_person']/my_dataset[person]['from_messages']\n",
    "        my_dataset[person]['shared_poi_message_proportion'] = my_dataset[person]['shared_receipt_with_poi']/my_dataset[person]['to_messages']\n",
    "        my_dataset[person]['bonus_to_salary'] = my_dataset[person]['bonus'] / my_dataset[person]['salary']\n",
    "        my_dataset[person]['bonus_to_total'] = my_dataset[person]['bonus'] / my_dataset[person]['total_payments']\n",
    "        \n",
    "    features_list = features_list + ['to_poi_message_proportion', 'from_poi_message_proportion', 'shared_poi_message_proportion', 'bonus_to_salary', 'bonus_to_total']\n",
    "\n",
    "    print 'Lista com novos atributos:', features_list\n",
    "    print\n",
    "    print 'Total de atributos:', len(features_list)\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Try a varity of classifiers\n",
    "Please name your classifier clf for easy export below.\n",
    "Note that if you want to do PCA or other multi-stage operations,\n",
    "you'll need to use Pipelines. For more info:\n",
    "http://scikit-learn.org/stable/modules/pipeline.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decision Tree Classifier\n",
      "\n",
      "\n",
      "{'skb__k': 5}\n",
      "\n",
      "0.4926190476190476\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree Classifier\n",
    "skb = SelectKBest()\n",
    "dtree = DecisionTreeClassifier()\n",
    "\n",
    "pipe = Pipeline(steps=[('skb', skb), ('dtree', dtree)])\n",
    "\n",
    "k = n_features = np.arange(1, len(features_list))\n",
    "\n",
    "clf = GridSearchCV(pipe, dict(skb__k=k), scoring='f1', cv =10)\n",
    "print\n",
    "print 'Decision Tree Classifier'\n",
    "print\n",
    "clf.fit(features, labels)\n",
    "\n",
    "print\n",
    "print clf.best_params_\n",
    "print\n",
    "print clf.best_score_\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AdaBoost Classifier\n",
      "\n",
      "\n",
      "0.4345238095238095\n",
      "\n",
      "{'skb__k': 13}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# AdaBoost Classifier\n",
    "skb = SelectKBest()\n",
    "ada = AdaBoostClassifier()\n",
    "\n",
    "pipe = Pipeline(steps=[('skb', skb), ('ada', ada)])\n",
    "\n",
    "k = n_features = np.arange(1, len(features_list))\n",
    "\n",
    "clf = GridSearchCV(pipe, dict(skb__k=k), scoring='f1', cv =10)\n",
    "print\n",
    "print 'AdaBoost Classifier'\n",
    "print\n",
    "clf.fit(features, labels)\n",
    "\n",
    "print\n",
    "print clf.best_score_\n",
    "print\n",
    "print clf.best_params_\n",
    "print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall \n",
    "using our testing script. Check the tester.py script in the final project\n",
    "folder for details on the evaluation method, especially the test_classifier\n",
    "function. Because of the small size of the dataset, the script uses\n",
    "stratified shuffle split cross validation. For more info: \n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example starting point. Try investigating other evaluation techniques!\n",
    "from sklearn.cross_validation import train_test_split\n",
    "features_train, features_test, labels_train, labels_test = \\\n",
    "    train_test_split(features, labels, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Decision Tree Classifier ###\n",
      "\n",
      "\n",
      "0.4235714285714286\n",
      "\n",
      "{'dtree__criterion': 'gini', 'skb__k': 5}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree Classifier\n",
    "skb = SelectKBest()\n",
    "dtree = DecisionTreeClassifier()\n",
    "\n",
    "pipe = Pipeline(steps=[('skb', skb), ('dtree', dtree)])\n",
    "\n",
    "k = n_features = [5]\n",
    "cri = ['gini','entropy']\n",
    "m_d = [None, 5, 10, 15, 20]\n",
    "m_f = [None, 'sqrt', 'log2', 'auto']\n",
    "m_s_s = [2, 4, 6, 8, 10, 20]\n",
    "\n",
    "dtree_clf = GridSearchCV(pipe, dict(skb__k=k, dtree__criterion=cri), scoring='f1', cv =10)\n",
    "\n",
    "print '### Decision Tree Classifier ###'\n",
    "print\n",
    "dtree_clf.fit(features, labels)\n",
    "print\n",
    "print dtree_clf.best_score_\n",
    "print\n",
    "print dtree_clf.best_params_\n",
    "print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print 'Iniciando Tester...'\n",
    "print\n",
    "tester.test_classifier(dtree_clf, my_dataset, features_list, folds = 200)\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### AdaBoost Classifier ###\n",
      "\n",
      "\n",
      "0.43999999999999995\n",
      "\n",
      "{'ada__n_estimators': 70, 'ada__base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), 'ada__learning_rate': 0.5, 'skb__k': 13}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# AdaBoost Classifier\n",
    "skb = SelectKBest()\n",
    "ada = AdaBoostClassifier()\n",
    "\n",
    "pipe = Pipeline(steps=[('skb', skb), ('ada', ada)])\n",
    "\n",
    "k = n_features = [13]\n",
    "b_est = [DecisionTreeClassifier(), RandomForestClassifier()]#, GaussianNB()]\n",
    "n_est = [30, 50, 70, 120]\n",
    "l_rat = [0.5, 1, 1.5, 2, 4]\n",
    "\n",
    "ada_clf = GridSearchCV(pipe, dict(skb__k=k, ada__base_estimator=b_est, ada__n_estimators=n_est, ada__learning_rate=l_rat), scoring='f1', cv =10)\n",
    "\n",
    "print '### AdaBoost Classifier ###'\n",
    "print\n",
    "ada_clf.fit(features, labels)\n",
    "print\n",
    "print ada_clf.best_score_\n",
    "print\n",
    "print ada_clf.best_params_\n",
    "print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print 'Iniciando Tester...'\n",
    "print\n",
    "tester.test_classifier(ada_clf, my_dataset, features_list, folds = 200)\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parâmetros otimizados para DecisionTree\n",
    "dtree_clf = Pipeline([('select_features', SelectKBest(k=10)),\n",
    "                      ('classify', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
    "                                                          max_features=None, max_leaf_nodes=None,\n",
    "                                                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                                                          min_samples_leaf=1, min_samples_split=2,\n",
    "                                                          min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
    "                                                          splitter='best')\n",
    "                      )])\n",
    "\n",
    "# Parâmetros otimizados para AdaBoost\n",
    "ada_clf = Pipeline([('select_features', SelectKBest(k=13)),\n",
    "                    ('classify', AdaBoostClassifier(base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini',\n",
    "                                                    max_depth=None,\n",
    "                                                    max_features=None, max_leaf_nodes=None,\n",
    "                                                    min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                                                    min_samples_leaf=1, min_samples_split=2,\n",
    "                                                    min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
    "                                                    splitter='best'), learning_rate=0.5, n_estimators=70)\n",
    "                    )])\n",
    "\n",
    "# Classificador escolhido\n",
    "clf = dtree_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "check your results. You do not need to change anything below, but make sure\n",
    "that the version of poi_id.py that you submit can be run on its own and\n",
    "generates the necessary .pkl files for validating your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester.dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "\n",
    "print 'Iniciando Tester...'\n",
    "print\n",
    "tester.main()\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
